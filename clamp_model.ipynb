{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "29b439fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "import cgcnn\n",
    "from cgcnn.data import CIFData\n",
    "from cgcnn.data import collate_pool, get_train_val_test_loader\n",
    "from cgcnn.model import CrystalGraphConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52a0565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dccde311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[1.00e+00 3.51e-05 0.00e+00]]\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "model, preprocess = clip.load(\"ViT-L/14@336px\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"octopussy\",\"cat\",\"A new design strategy for high-performance organic cathode active materials for lithium-ion batteries is presented.X-ray diffraction measurements and sorption experiments demonstrated that the intercolumnar spaces in PCT-1 can incorporate various molecules accompanied by lattice expansion.\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed920a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch/_tensor_str.py:115: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  nonzero_finite_vals = torch.masked_select(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[21.3438, 11.0859,  2.9668]], device='mps:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8857447a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0d5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CIFData(\"/Users/neelredkar/Documents/Git/cgcnn/data/sample-regression\")\n",
    "s, _, _ = data[0]\n",
    "cif_encoder = CrystalGraphConvNet(s[0].shape[-1], s[1].shape[-1],\n",
    "                                n_conv=3,\n",
    "                                n_h=2,\n",
    "                                output_dim=image_features.shape[-1],\n",
    "                                classification=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5d1bc8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7827\n",
      "0.0254\n",
      "0.02936\n",
      "0.02512\n",
      "0.03284\n",
      "0.0371\n",
      "0.03192\n",
      "0.03555\n"
     ]
    }
   ],
   "source": [
    "text = [\n",
    "  \"\"\"An octopus (pl: octopuses or octopodes, see below for variants) is a soft-bodied, eight-limbed mollusc of the order Octopoda (/ɒkˈtɒpədə/, ok-TOP-ə-də[3]). The order consists of some 300 species and is grouped within the class Cephalopoda with squids, cuttlefish, and nautiloids. Like other cephalopods, an octopus is bilaterally symmetric with two eyes and a beaked mouth at the center point of the eight limbs.[a] The soft body can radically alter its shape, enabling octopuses to squeeze through small gaps. They trail their eight appendages behind them as they swim. The siphon is used both for respiration and for locomotion, by expelling a jet of water. Octopuses have a complex nervous system and excellent sight, and are among the most intelligent and behaviourally diverse of all invertebrates. Octopuses inhabit various regions of the ocean, including coral reefs, pelagic waters, and the seabed; some live in the intertidal zone and others at abyssal depths. Most species grow quickly, mature early, and are short-lived. In most species, the male uses a specially adapted arm to deliver a bundle of sperm directly into the female's mantle cavity, after which he becomes senescent and dies, while the female deposits fertilised eggs in a den and cares for them until they hatch, after which she also dies. Strategies to defend themselves against predators include the expulsion of ink, the use of camouflage and threat displays, the ability to jet quickly through the water and hide, and even deceit. All octopuses are venomous, but only the blue-ringed octopuses are known to be deadly to humans.\"\"\",\n",
    "  'The title complex was synthesized in 41.6% yield by reactions between Os3(CO)11(CH3CN) and 2,4,6-tri­methyl­hexa­hydro-1,3,5-triazine.Each Os atom exhibits a pseudo-octa­hedral coordination environment, discounting the bridging Os—Os bond.',\n",
    "  'The molecular salt, C23H26N2O2+Cl, was obtained from 1-isobutyl-8,9-dimeth­oxy-3-phenyl-5,6-di­hydro­imidazo[5,1-a]iso­quinoline.In the crystal structure, centrosymmetric dimers are formed by N—HCl and C—HCl hydrogen bonds.',\n",
    "  'The title compound, C16H20N4, was synthesized by cyanation of brom­hexine.The substituted aniline and cyclo­hexane rings are inclined to one another by 37.26 (6)in one mol­ecule and by 22.84 (7)in the other.',\n",
    "  'Your purchase has been completed.Your documents are now available to view.Your purchase has been completed.Your documents are now available to view.',\n",
    "  'Monomeric boroles have been gaining attention as reagents for the synthesis of heterocycles due to their ability to insert atoms into the BC4 ring in a single step.This work demonstrates that insertion chemistry is possible with Diels–Alder dimeric boroles.',\n",
    "  'Deep-blue thermally activated delayed fluorescence (TADF) emitters are promising alternatives for conventional fluorescence and phosphorescence materials.Four new donor–acceptor (D–A)-type TADF molecules incorporating phenazasiline, phenazagermine, and tetramethylcarbazole as weak D units were designed and synthesized.Photophysical investigation revealed that phenazasiline and phenazagermine-based emitters concurrently exhibit blue TADF emissions.',\n",
    "  'Silyl, silylene and silene complexes were accessed via reactions of [(dmpe)2MnH(C2H4)] (1) with hydrosilanes, in some cases followed by ethylene.'\n",
    "]\n",
    "# get longest text in batch\n",
    "context_length = max([len(c) for c in text])\n",
    "context_length = int(np.ceil(context_length / 77) * 77)\n",
    "\n",
    "tokens = clip.tokenize(text, context_length=context_length).reshape(len(text), -1,77).to(device)\n",
    "#print(tokens.shape)\n",
    "\n",
    "embeddings = []\n",
    "# run through clip\n",
    "with torch.no_grad():\n",
    "  for sample in tokens:\n",
    "      ctx = model.encode_text(sample)\n",
    "      # average ctx\n",
    "      ctx = torch.mean(ctx, dim=0)\n",
    "      embeddings.append(ctx)\n",
    "  embeddings = torch.stack(embeddings)\n",
    "  image_features = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "  embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)\n",
    "\n",
    "  # cosine similarity as logits\n",
    "  logit_scale = model.logit_scale.exp()\n",
    "  logits_per_image = logit_scale * image_features @ embeddings.t()\n",
    "  probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "  for i in probs[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "755d44d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4]\n",
    "x.append(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4ed31c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 77])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_cif_embeddings = cif_encoder(*collate_pool(data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8213836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1f33e4610>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1f33e6b60>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87c6cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, val, train ratio is 0.1, 0.1, 0.8\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "        train_size=len(data),\n",
    "        test_size=int(len(data)*0.1),\n",
    "        val_size=int(len(data)*0.1),\n",
    "        dataset=data,\n",
    "        collate_fn=collate_pool,\n",
    "        batch_size=1,\n",
    "        return_test=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "079cfe7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cgcnn' from '/Users/neelredkar/Documents/Git/clamp/cgcnn/__init__.py'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(cgcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d2ad8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/10 (0%)]\tLoss: 0.007412\tVal Loss: 0.040125\n",
      "Train Epoch: 0 [0/10 (0%)]\tLoss: 0.040125\tVal Loss: 0.004001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                         val_loss \u001b[39m=\u001b[39m loss_func(cif_embeddings\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device), text_embeddings\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m                 \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain Epoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{:.0f}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m)]\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mLoss: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mVal Loss: \u001b[39m\u001b[39m{:.6f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m                     epoch, batch_idx\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(targets), \u001b[39mlen\u001b[39m(train_loader),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m                     \u001b[39m100.\u001b[39m \u001b[39m*\u001b[39m batch_idx \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader), loss\u001b[39m.\u001b[39mitem(), val_loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m train(\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb Cell 13\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(cif_embeddings\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device), text_embeddings\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# check validation loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/neelredkar/Documents/Git/clamp/clamp_model.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/optim/adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    363\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[1;32m    367\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"mps\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_func(feat1, feat2):\n",
    "    # minimize average magnitude of cosine similarity\n",
    "    return F.cosine_similarity(feat1, feat2).abs().mean()\n",
    "def encode_text(targets):\n",
    "    context_length = max([len(c) for c in targets])\n",
    "    context_length = int(np.ceil(context_length / 77) * 77)\n",
    "\n",
    "    tokens = clip.tokenize(targets, context_length=context_length).reshape(len(targets), -1,77).to(device)\n",
    "    embeddings = []\n",
    "    for sample in tokens:\n",
    "        ctx = model.encode_text(sample)\n",
    "        # average ctx\n",
    "        ctx = torch.mean(ctx, dim=0)\n",
    "        embeddings.append(ctx)\n",
    "    text_embeddings = torch.stack(embeddings)\n",
    "    return text_embeddings\n",
    "\n",
    "# model = text encoder (unused image encoder)\n",
    "# cif_encoder\n",
    "model = model.float()\n",
    "cif_encoder = cif_encoder.float()\n",
    "cif_encoder.train()\n",
    "model.train()\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        scheduler.step()\n",
    "        for batch_idx, (inputs, targets, _) in enumerate(train_loader):\n",
    "            inputs = (Variable(inputs[0].float()),\n",
    "                         Variable(inputs[1].float()),\n",
    "                         inputs[2],\n",
    "                         [crys_idx for crys_idx in inputs[3]])\n",
    "            cif_embeddings = cif_encoder(*inputs)\n",
    "            text_embeddings = encode_text(targets)\n",
    "            cif_embeddings = cif_embeddings / cif_embeddings.norm(dim=1, keepdim=True)\n",
    "            text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)\n",
    "            #convert text embeddings to list\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(cif_embeddings.float().to(device), text_embeddings.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 1 == 0:\n",
    "                # check validation loss\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch_idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "                        inputs = (Variable(inputs[0].float()),\n",
    "                                    Variable(inputs[1].float()),\n",
    "                                    inputs[2],\n",
    "                                    [crys_idx for crys_idx in inputs[3]])\n",
    "                        cif_embeddings = cif_encoder(*inputs)\n",
    "                        text_embeddings = encode_text(targets)\n",
    "                        cif_embeddings = cif_embeddings / cif_embeddings.norm(dim=1, keepdim=True)\n",
    "                        text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)\n",
    "                        val_loss = loss_func(cif_embeddings.float().to(device), text_embeddings.float())\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tVal Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx*len(targets), len(train_loader),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(), val_loss.item()))\n",
    "train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a691368",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (inputs, targets, _) in enumerate(train_loader):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4338cea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 768])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = collate_pool(data)[0]\n",
    "model(*x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8689fac7afca8cfd8aa85671ea19899a8f2204abddab2f8fabdacf0ec105764e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
