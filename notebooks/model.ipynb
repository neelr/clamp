{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:13:13.595212Z",
     "iopub.status.busy": "2023-02-12T08:13:13.594772Z",
     "iopub.status.idle": "2023-02-12T08:13:39.320493Z",
     "shell.execute_reply": "2023-02-12T08:13:39.319286Z",
     "shell.execute_reply.started": "2023-02-12T08:13:13.595113Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:13:39.324795Z",
     "iopub.status.busy": "2023-02-12T08:13:39.324474Z",
     "iopub.status.idle": "2023-02-12T08:13:42.761130Z",
     "shell.execute_reply": "2023-02-12T08:13:42.759953Z",
     "shell.execute_reply.started": "2023-02-12T08:13:39.324763Z"
    }
   },
   "outputs": [],
   "source": [
    "!curl https://doggo.ninja/6A0kZE.zip -o cgcnn.zip\n",
    "!mkdir cgcnn\n",
    "!unzip -o cgcnn.zip -d cgcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:13:42.765210Z",
     "iopub.status.busy": "2023-02-12T08:13:42.764885Z",
     "iopub.status.idle": "2023-02-12T08:14:59.066708Z",
     "shell.execute_reply": "2023-02-12T08:14:59.065465Z",
     "shell.execute_reply.started": "2023-02-12T08:13:42.765177Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pymatgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:14:59.070875Z",
     "iopub.status.busy": "2023-02-12T08:14:59.070530Z",
     "iopub.status.idle": "2023-02-12T08:15:02.042145Z",
     "shell.execute_reply": "2023-02-12T08:15:02.041156Z",
     "shell.execute_reply.started": "2023-02-12T08:14:59.070842Z"
    }
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from random import sample\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import clip\n",
    "import cgcnn\n",
    "from cgcnn.data import CIFData\n",
    "from cgcnn.data import collate_pool, get_train_val_test_loader\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:15:02.044248Z",
     "iopub.status.busy": "2023-02-12T08:15:02.043587Z",
     "iopub.status.idle": "2023-02-12T08:15:02.054035Z",
     "shell.execute_reply": "2023-02-12T08:15:02.053000Z",
     "shell.execute_reply.started": "2023-02-12T08:15:02.044217Z"
    }
   },
   "outputs": [],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:15:02.056956Z",
     "iopub.status.busy": "2023-02-12T08:15:02.055761Z",
     "iopub.status.idle": "2023-02-12T08:15:40.060072Z",
     "shell.execute_reply": "2023-02-12T08:15:40.059017Z",
     "shell.execute_reply.started": "2023-02-12T08:15:02.056919Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-L/14@336px\", device=device)\n",
    "\n",
    "text = clip.tokenize([\"octopussy\",\"cat\",\"A new design strategy for high-performance organic cathode active materials for lithium-ion batteries is presented.X-ray diffraction measurements and sorption experiments demonstrated that the intercolumnar spaces in PCT-1 can incorporate various molecules accompanied by lattice expansion.\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    #logits_per_image, logits_per_text = model(image, text)\n",
    "    #probs = logits_per_image.softmax(dim=-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cifs/id_prop.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[0].apply(lambda x: os.path.exists(f\"cifs/{x}.cif\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cifs/id_prop.csv\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:15:40.062119Z",
     "iopub.status.busy": "2023-02-12T08:15:40.061731Z",
     "iopub.status.idle": "2023-02-12T08:15:40.804072Z",
     "shell.execute_reply": "2023-02-12T08:15:40.803022Z",
     "shell.execute_reply.started": "2023-02-12T08:15:40.062077Z"
    }
   },
   "outputs": [],
   "source": [
    "data = CIFData(\"cif_photocatalyst\")\n",
    "# run only if you have invalid CIF file errors - this will find & delete them\n",
    "#data.find_errors(write=False)\n",
    "s, _, _ = data[0]\n",
    "cif_encoder = CrystalGraphConvNet(s[0].shape[-1], s[1].shape[-1],\n",
    "                                n_conv=3,\n",
    "                                n_h=2,\n",
    "                                output_dim=text_features.shape[-1],\n",
    "                                classification=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:15:40.806405Z",
     "iopub.status.busy": "2023-02-12T08:15:40.805424Z",
     "iopub.status.idle": "2023-02-12T08:15:40.817456Z",
     "shell.execute_reply": "2023-02-12T08:15:40.815562Z",
     "shell.execute_reply.started": "2023-02-12T08:15:40.806364Z"
    }
   },
   "outputs": [],
   "source": [
    "# test, val, train ratio is 0.1, 0.1, 0.8\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "        train_ratio=1,\n",
    "        val_ratio=0,\n",
    "        test_ratio=0,\n",
    "        dataset=data,\n",
    "        collate_fn=collate_pool,\n",
    "        batch_size=1,\n",
    "        return_test=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cif_encoder.load_state_dict(torch.load(\"checkpoints/checkpoint_cif_1.pt\",  map_location=torch.device('cpu'))[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"checkpoints/checkpoint_clip_1.pt\",  map_location=torch.device('cpu'))[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:15:40.820112Z",
     "iopub.status.busy": "2023-02-12T08:15:40.819316Z",
     "iopub.status.idle": "2023-02-12T08:15:40.836031Z",
     "shell.execute_reply": "2023-02-12T08:15:40.835038Z",
     "shell.execute_reply.started": "2023-02-12T08:15:40.820069Z"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(cgcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:15:40.839884Z",
     "iopub.status.busy": "2023-02-12T08:15:40.839548Z",
     "iopub.status.idle": "2023-02-12T08:15:40.848032Z",
     "shell.execute_reply": "2023-02-12T08:15:40.846876Z",
     "shell.execute_reply.started": "2023-02-12T08:15:40.839833Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "os.mkdir(\"checkpoints\")\n",
    "os.mkdir(\"clamp_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-12T08:19:38.522603Z",
     "iopub.status.busy": "2023-02-12T08:19:38.522231Z",
     "iopub.status.idle": "2023-02-12T08:21:43.003692Z",
     "shell.execute_reply": "2023-02-12T08:21:43.001931Z",
     "shell.execute_reply.started": "2023-02-12T08:19:38.522567Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "import torch.nn.functional as F\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "def loss_func(feat1, feat2):\n",
    "    # minimize average magnitude of cosine similarity\n",
    "    logits = feat1 @ feat2.T\n",
    "    feat1_similarity = feat1 @ feat1.T\n",
    "    feat2_similarity = feat2 @ feat2.T\n",
    "    targets = F.softmax(\n",
    "            (feat1_similarity + feat2_similarity) / 2, dim=-1\n",
    "        )\n",
    "    feat1_loss = cross_entropy(logits, targets, reduction='none')\n",
    "    feat2_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "    loss =  (feat1_loss + feat2_loss) / 2.0\n",
    "    return loss\n",
    "def encode_text(targets):\n",
    "    context_length = max([len(c) for c in targets])\n",
    "    context_length = int(np.ceil(context_length / 77) * 77)\n",
    "\n",
    "    tokens = clip.tokenize(targets, context_length=context_length).reshape(len(targets), -1,77).to(device)\n",
    "    embeddings = []\n",
    "    for sample in tokens:\n",
    "        ctx = model.encode_text(sample)\n",
    "        # average ctx\n",
    "        ctx = torch.mean(ctx, dim=0)\n",
    "        embeddings.append(ctx)\n",
    "    text_embeddings = torch.stack(embeddings)\n",
    "    return text_embeddings\n",
    "\n",
    "# model = text encoder (unused image encoder)\n",
    "# cif_encoder\n",
    "model = model.cuda()\n",
    "cif_encoder = cif_encoder.cuda()\n",
    "model = model.float()\n",
    "cif_encoder = cif_encoder.float()\n",
    "cif_encoder.train()\n",
    "model.train()\n",
    "least_loss = float('inf')\n",
    "least_val_loss = float('inf')\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (inputs, targets, _) in enumerate(train_loader):\n",
    "            inputs = (Variable(inputs[0].cuda(non_blocking=False).float()),\n",
    "                         Variable(inputs[1].cuda(non_blocking=False).float()),\n",
    "                         inputs[2].cuda(non_blocking=False),\n",
    "                         [crys_idx.cuda(non_blocking=False) for crys_idx in inputs[3]])\n",
    "            cif_embeddings = cif_encoder(*inputs)\n",
    "            text_embeddings = encode_text(targets)\n",
    "            cif_embeddings = cif_embeddings / cif_embeddings.norm(dim=1, keepdim=True)\n",
    "            text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)\n",
    "            #convert text embeddings to list\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_func(cif_embeddings.float().to(device), text_embeddings.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            txt = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx, len(train_loader),\n",
    "                    100. * batch_idx / len(train_loader), loss.item())\n",
    "            print(txt)\n",
    "            if batch_idx % 15 == 0:\n",
    "                scheduler.step(loss)\n",
    "                # check validation loss\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch_idx, (inputs, targets, _) in enumerate(val_loader):\n",
    "                        inputs = (Variable(inputs[0].cuda(non_blocking=False).float()),\n",
    "                                    Variable(inputs[1].cuda(non_blocking=False).float()),\n",
    "                                    inputs[2].cuda(non_blocking=False),\n",
    "                                    [crys_idx.cuda(non_blocking=False) for crys_idx in inputs[3]])\n",
    "                        cif_embeddings = cif_encoder(*inputs)\n",
    "                        text_embeddings = encode_text(targets)\n",
    "                        cif_embeddings = cif_embeddings / cif_embeddings.norm(dim=1, keepdim=True)\n",
    "                        text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)\n",
    "                        val_loss = loss_func(cif_embeddings.float().to(device), text_embeddings.float())\n",
    "\n",
    "                # save checkpoints with loss & epoch metrics\n",
    "                if least_val_loss > val_loss:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss.item(),\n",
    "                        'val_loss': val_loss.item()\n",
    "                        }, 'checkpoints/checkpoint_clip_val_least.pt')\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': cif_encoder.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss.item(),\n",
    "                        'val_loss': val_loss.item()\n",
    "                        }, 'checkpoints/checkpoint_cif_val_least.pt')\n",
    "                if least_loss > loss:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss.item(),\n",
    "                        'val_loss': val_loss.item()\n",
    "                        }, 'checkpoints/checkpoint_clip_least.pt')\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': cif_encoder.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss.item(),\n",
    "                        'val_loss': val_loss.item()\n",
    "                        }, 'checkpoints/checkpoint_cif_least.pt')\n",
    "                    \n",
    "                txt = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tVal Loss: {:.6f}'.format(\n",
    "                    epoch, batch_idx, len(train_loader),\n",
    "                    100. * batch_idx / len(train_loader), loss.item(), val_loss.item())\n",
    "                print(txt)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "            'val_loss': val_loss.item()\n",
    "            }, 'checkpoints/checkpoint_clip_{}.pt'.format(epoch))\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': cif_encoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "            'val_loss': val_loss.item()\n",
    "            }, 'checkpoints/checkpoint_cif_{}.pt'.format(epoch))\n",
    "\n",
    "train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-02-12T03:50:54.499228Z",
     "iopub.status.idle": "2023-02-12T03:50:54.499990Z",
     "shell.execute_reply": "2023-02-12T03:50:54.499763Z",
     "shell.execute_reply.started": "2023-02-12T03:50:54.499739Z"
    }
   },
   "outputs": [],
   "source": [
    "# save models to clamp_weights folder\n",
    "torch.save(model.state_dict(), \"clamp_weights/text_encoder.pt\")\n",
    "torch.save(cif_encoder.state_dict(), \"clamp_weights/cif_encoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0,1][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\"\n",
    "model.eval()\n",
    "cif_encoder.eval()\n",
    "vectors = {}\n",
    "for batch_idx, (inputs, targets, _) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "    inputs = (Variable(inputs[0].float()),\n",
    "                 Variable(inputs[1].float()),\n",
    "                 inputs[2],\n",
    "                 [crys_idx for crys_idx in inputs[3]])\n",
    "    with torch.no_grad():\n",
    "        cif_embedding0 = cif_encoder(*inputs)[:1]\n",
    "        cif_embedding1 = cif_encoder(*inputs)[1:]\n",
    "        vectors[targets[0]] = cif_embedding0\n",
    "        vectors[targets[1]] = cif_embedding1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('text2embedding.pkl', 'wb') as handle:\n",
    "    pickle.dump(vectors, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cpu\"\n",
    "model.eval()\n",
    "cif_encoder.eval()\n",
    "total = 0\n",
    "gone = 0\n",
    "for batch_idx, (inputs, targets, _) in enumerate(train_loader):\n",
    "    inputs = (Variable(inputs[0].float()),\n",
    "                 Variable(inputs[1].float()),\n",
    "                 inputs[2],\n",
    "                 [crys_idx for crys_idx in inputs[3]])\n",
    "    with torch.no_grad():\n",
    "        gone+=1\n",
    "        cif_embedding0 = cif_encoder(*inputs)\n",
    "        cif_embedding1 = torch.cat([list(vectors.values())[0], cif_embedding0], dim=-2)\n",
    "        print(cif_embedding1.shape)\n",
    "        text_embeddings = encode_text([\"photocatalyst methane adsorption conversion artificial photosynthesis\"])\n",
    "        #cif_embeddings = cif_embeddings / cif_embeddings.norm(dim=1, keepdim=True)\n",
    "        text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)\n",
    "        loss0 = loss_func(cif_embedding1.float().to(device), text_embeddings.float())\n",
    "        probs0 = loss0.softmax(dim=-1).cpu().numpy()\n",
    "        #loss1 = loss_func(text_embeddings.float(), cif_embedding1.float().to(device))\n",
    "        #probs1 = loss1.softmax(dim=-1).cpu().numpy()\n",
    "        if np.argmax(probs0) == 1:\n",
    "            total+=1\n",
    "        print(probs0)\n",
    "        print(f\"{total}/{gone} {(total/gone)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "with torch.no_grad():\n",
    "    text_embeddings = encode_text([\"2d flat photocatalyst methane adsorption conversion artificial photosynthesis with visible light\"])\n",
    "    cif_embeddings = torch.cat(list(crystals.values()), dim=-2)\n",
    "    cif_embeddings = cif_embeddings / cif_embeddings.norm(dim=1, keepdim=True)\n",
    "    text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)\n",
    "    loss = loss_func(text_embeddings.float(), cif_embeddings.float().to(device))\n",
    "    probs = loss.softmax(dim=-1).cpu().numpy()\n",
    "    idxs = np.argpartition(probs, -10)[-10:]\n",
    "    print(np.argmin(probs))\n",
    "    print(list(crystals.keys())[np.argmin(probs)])\n",
    "    for idx in idxs[::-1]:\n",
    "        print(probs[idx])\n",
    "        print(list(crystals.keys())[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pca = TSNE(n_components=2)\n",
    "reduced = pca.fit_transform(torch.cat(list(vectors.values()), dim=-2))\n",
    "\n",
    "# We need a 2 x 944 array, not 944 by 2 (all X coordinates in one list)\n",
    "t = reduced.transpose()\n",
    "plt.title(\"Final Latent Space Diagram (77 dimensional)\")\n",
    "for idx in range(len(t[0])):\n",
    "    colort = \"blue\"\n",
    "    label = \"non-photocatalyst\"\n",
    "    if \"photo\" in list(vectors.keys())[idx]:\n",
    "        colort=\"red\"\n",
    "        label=\"photocatalyst\"\n",
    "    plt.scatter(t[0][idx], t[1][idx], color=colort, alpha=0.1)\n",
    "    \n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Photocatalyst',\n",
    "                          markerfacecolor='red', markersize=10), Line2D([0], [0], marker='o', color='w', label='Non-Catalyst',\n",
    "                          markerfacecolor='blue', markersize=10)]\n",
    "\n",
    "plt.legend(handles=legend_elements)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text2embedding.pkl\", 'rb') as f:\n",
    "    vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('logs/logs-new-epoch1.txt') as f:\n",
    "    contents = f.read()\n",
    "    matches = re.findall(r\"Loss: (.*)\\n\", contents)\n",
    "    matches = filter(lambda x: not \"Val\" in x, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [float(i) for i in matches]\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "plt.plot(list(range(len(matches)))[49:], moving_average(matches, 50), 'b', label = 'Validation acc')\n",
    "\n",
    "plt.title(\"Loss Over Batches (2 Epochs)\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cosine Similarity Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(\"cif_csd\") if isfile(join(\"cif_csd\", f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifs = list(filter(lambda x: \"cif\" in x, onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifs = [i.split(\".cif\")[0] for i in cifs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"file\":cifs, \"text\":list(range(len(cifs)))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cif_csd/id_prop.csv\", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CIFData(\"cif_csd\")\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "        train_ratio=0.14,\n",
    "        val_ratio=0,\n",
    "        test_ratio=0,\n",
    "        dataset=data,\n",
    "        collate_fn=collate_pool,\n",
    "        batch_size=1,\n",
    "        return_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "crystals = {}\n",
    "#data.find_errors(write=True)\n",
    "for (inputs, targets, name) in tqdm(train_loader, total=len(train_loader)):\n",
    "    crystals[name[0]] = cif_encoder(*inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
